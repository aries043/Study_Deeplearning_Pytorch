{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Residual Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x # Skip Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **ResNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv6Res(nn.Module):\n",
    "    def __init__(self, n_class=10):\n",
    "        super().__init__()\n",
    "        self.name = 'conv6res'\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), # 입력채널(RGB), 출력채널, 커널 크기(3x3), Stride, Padding\n",
    "            nn.BatchNorm2d(32), # 배치 정규화(입력 데이터의 분포를 정규화함)\n",
    "            nn.ReLU(), # 활성화함수 ReLU로 비선형성 부여\n",
    "\n",
    "            ResidualBlock(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(),\n",
    "\n",
    "                    nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            ),\n",
    "\n",
    "            ResidualBlock(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(),\n",
    "\n",
    "                    nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(),\n",
    "\n",
    "                    nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            ),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*32*32, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 0, train loss: 1.744462, train acc: 0.377404, valid loss: 1.511918, valid acc: 0.455329\n",
      "Epoch: 10, train loss: 0.467130, train acc: 0.835292, valid loss: 1.190502, valid acc: 0.655248\n",
      "Epoch: 20, train loss: 0.172632, train acc: 0.941440, valid loss: 2.087788, valid acc: 0.647636\n",
      "Epoch: 30, train loss: 0.099709, train acc: 0.967504, valid loss: 2.735033, valid acc: 0.641426\n",
      "===== Test Start =====\n",
      "Test loss: 3.114580, Test acc: 0.641827\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Downloading CIFAR-10 Dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "cifar_tr = datasets.CIFAR10(root=os.getcwd(), train=True, download=True, transform=transform)\n",
    "cifar_test = datasets.CIFAR10(root=os.getcwd(), train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into train set and validation set\n",
    "def split_train_valid(dataset, valid_ratio=0.1):\n",
    "    data_size = len(dataset)\n",
    "    indices = list(range(data_size))\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_point = int(np.floor(valid_ratio*data_size))\n",
    "    val_index, train_index = indices[:split_point-1], indices[split_point:]\n",
    "\n",
    "    train = torch.utils.data.Subset(dataset, train_index)\n",
    "    valid = torch.utils.data.Subset(dataset, val_index)\n",
    "\n",
    "    return train, valid\n",
    "\n",
    "cifar_train, cifar_valid = split_train_valid(dataset=cifar_tr)\n",
    "\n",
    "# Make DataLoaders for train/validation/test datasets\n",
    "cifar_loaders = [DataLoader(dataset=d, batch_size=128, shuffle=True, drop_last=True) for d in [cifar_train, cifar_valid, cifar_test]]\n",
    "\n",
    "# Define model to train\n",
    "model = Conv6Res()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Use GPU if available (cuda if not)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 40\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"valid_loss\":[], \"valid_acc\":[]} # record of loss and accuracy in each epoch for plotting later\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.train()\n",
    "    for (x, y) in cifar_loaders[0]: # cifar_loaders[0] is train set DataLoader\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        hypothesis = model(x)\n",
    "        cost = loss(hypothesis, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += cost.to(device).item()\n",
    "        train_acc += (hypothesis.argmax(1)==y).type(torch.float).to(device).mean().item()\n",
    "\n",
    "    train_loss /= len(cifar_loaders[0]) # len(DataLoader) is batch size (ie, 128)\n",
    "    train_acc /= len(cifar_loaders[0])\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in cifar_loaders[1]: # Validation set DataLoader\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            hypothesis = model(x)\n",
    "            cost = loss(hypothesis, y)\n",
    "\n",
    "            valid_loss += cost.to(device).item()\n",
    "            valid_acc += (hypothesis.argmax(1)==y).type(torch.float).to(device).mean().item()\n",
    "    \n",
    "    valid_loss /= len(cifar_loaders[1]) # len(DataLoader) is batch size\n",
    "    valid_acc /= len(cifar_loaders[1])\n",
    "    history[\"valid_loss\"].append(valid_loss)\n",
    "    history[\"valid_acc\"].append(valid_acc)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, train loss: {train_loss:>6f}, train acc: {train_acc:>3f}, valid loss: {valid_loss:>6f}, valid acc: {valid_acc:>3f}\")\n",
    "\n",
    "# Test the model on the test set\n",
    "print(\"===== Test Start =====\")\n",
    "test_loss, test_acc = 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (x, y) in cifar_loaders[2]:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        hypothesis = model(x)\n",
    "        cost = loss(hypothesis, y)\n",
    "\n",
    "        test_loss += cost.to(device).item()\n",
    "        test_acc += (hypothesis.argmax(1)==y).type(torch.float).to(device).mean().item()\n",
    "\n",
    "test_loss /= len(cifar_loaders[2])\n",
    "test_acc /= len(cifar_loaders[2])\n",
    "print(f\"Test loss: {test_loss:>6f}, Test acc: {test_acc:>6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

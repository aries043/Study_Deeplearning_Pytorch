{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Char RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**훈련 데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str + label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "input_size = vocab_size #  입력은 원-핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합의 크기여야만 함\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "index_to_char={}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "# nn.RNN()은 3차원 텐서를 입력받기 때문에 배치차원 추가\n",
    "\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 벡터로 바꿔주기\n",
    "\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aries\\AppData\\Local\\Temp\\ipykernel_4216\\2348034151.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  X = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**모델 구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net,self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape) # 배치 차원, 시점, 출력의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, input_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape) # 정확도 측정 시에는 이 형태로 계산산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.6163734197616577 prediction:  [[2 2 2 3 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eeell\n",
      "1 loss:  1.3258720636367798 prediction:  [[4 3 3 3 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pllll\n",
      "2 loss:  1.10878586769104 prediction:  [[4 4 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppll!\n",
      "3 loss:  0.9173469543457031 prediction:  [[4 4 3 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppl!!\n",
      "4 loss:  0.7490520477294922 prediction:  [[4 4 3 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppl!!\n",
      "5 loss:  0.6088422536849976 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.5051730275154114 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.4215237498283386 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.3378075957298279 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.2580251395702362 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.18892666697502136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.13598929345607758 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.09494993090629578 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.06558787822723389 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.04468059539794922 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.030491996556520462 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.021280132234096527 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.015340435318648815 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.011416472494602203 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.008722103200852871 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.006803191266953945 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.005399442743510008 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.004354668315500021 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.0035678353160619736 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.0029693362303078175 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.002509527374058962 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.0021525807678699493 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.0018723709508776665 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.001649707555770874 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0014707075897604227 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.0013251259224489331 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0012053193058818579 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0011058261152356863 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.0010222021955996752 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.0009513575350865722 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.0008907713927328587 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.0008385645924136043 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.0007932145381346345 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0007536027696914971 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0007187057635746896 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0006878572748973966 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0006604144000448287 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0006358056562021375 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0006137931486591697 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0005938528920523822 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0005758182378485799 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0005594270769506693 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0005444415146484971 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0005307183600962162 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0005180671578273177 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0005064403521828353 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0004956473712809384 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0004857358580920845 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.00047641986748203635 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.0004677472170442343 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0004596462531480938 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0004520694783423096 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0004449453263077885 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.00043822621228173375 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.00043191207805648446 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0004258599947206676 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.00042021297849714756 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0004147803410887718 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.00040968129178509116 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0004047728725709021 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.00040005496703088284 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.00039559914148412645 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0003912624379154295 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0003871878725476563 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0003831847570836544 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.00037939604953862727 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.00037572649307549 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.000372128386516124 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.0003686731797643006 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.000365361018339172 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.00036207257653586566 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0003589509869925678 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.00035587704041972756 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0003528983797878027 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0003500388120301068 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0003472507814876735 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0003445103648118675 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0003417937841732055 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0003391486534383148 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.00033655116567388177 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0003340728289913386 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.00033159449230879545 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0003291876055300236 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0003267807187512517 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0003244929830543697 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0003221814113203436 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.00031998896156437695 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0003177727048750967 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.00031560403294861317 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.00031338774715550244 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.00031133825541473925 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.00030924109159968793 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0003071915707550943 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0003052373940590769 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0003032355452887714 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # Batch 차원 제거를 위해 view 사용\n",
    "    loss.backward()\n",
    "    optimizer.step() # optimizer의 파라미터 업데이트\n",
    "\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 각 시점별 5차원 벡터에 대해서 가장 높은 값의 인덱스 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Char RNN(More Data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**훈련 데이터 전처리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0, 's': 1, ' ': 2, 'l': 3, ',': 4, 'f': 5, 'g': 6, 'u': 7, 'a': 8, 'k': 9, 'w': 10, 'n': 11, 'b': 12, 'i': 13, 'y': 14, \"'\": 15, 'p': 16, 'c': 17, 't': 18, 'd': 19, 'o': 20, 'r': 21, 'h': 22, '.': 23, 'm': 24}\n"
     ]
    }
   ],
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {c : i for i, c in enumerate(char_set)}\n",
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10 # 임의 숫자(10의 단위로 샘플을 잘라서 데이터를 만듦)\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan  ->  f you want\n",
      "1 f you want  ->   you want \n",
      "2  you want   ->  you want t\n",
      "3 you want t  ->  ou want to\n",
      "4 ou want to  ->  u want to \n",
      "5 u want to   ->   want to b\n",
      "6  want to b  ->  want to bu\n",
      "7 want to bu  ->  ant to bui\n",
      "8 ant to bui  ->  nt to buil\n",
      "9 nt to buil  ->  t to build\n",
      "10 t to build  ->   to build \n",
      "11  to build   ->  to build a\n",
      "12 to build a  ->  o build a \n",
      "13 o build a   ->   build a s\n",
      "14  build a s  ->  build a sh\n",
      "15 build a sh  ->  uild a shi\n",
      "16 uild a shi  ->  ild a ship\n",
      "17 ild a ship  ->  ld a ship,\n",
      "18 ld a ship,  ->  d a ship, \n",
      "19 d a ship,   ->   a ship, d\n",
      "20  a ship, d  ->  a ship, do\n",
      "21 a ship, do  ->   ship, don\n",
      "22  ship, don  ->  ship, don'\n",
      "23 ship, don'  ->  hip, don't\n",
      "24 hip, don't  ->  ip, don't \n",
      "25 ip, don't   ->  p, don't d\n",
      "26 p, don't d  ->  , don't dr\n",
      "27 , don't dr  ->   don't dru\n",
      "28  don't dru  ->  don't drum\n",
      "29 don't drum  ->  on't drum \n",
      "30 on't drum   ->  n't drum u\n",
      "31 n't drum u  ->  't drum up\n",
      "32 't drum up  ->  t drum up \n",
      "33 t drum up   ->   drum up p\n",
      "34  drum up p  ->  drum up pe\n",
      "35 drum up pe  ->  rum up peo\n",
      "36 rum up peo  ->  um up peop\n",
      "37 um up peop  ->  m up peopl\n",
      "38 m up peopl  ->   up people\n",
      "39  up people  ->  up people \n",
      "40 up people   ->  p people t\n",
      "41 p people t  ->   people to\n",
      "42  people to  ->  people tog\n",
      "43 people tog  ->  eople toge\n",
      "44 eople toge  ->  ople toget\n",
      "45 ople toget  ->  ple togeth\n",
      "46 ple togeth  ->  le togethe\n",
      "47 le togethe  ->  e together\n",
      "48 e together  ->   together \n",
      "49  together   ->  together t\n",
      "50 together t  ->  ogether to\n",
      "51 ogether to  ->  gether to \n",
      "52 gether to   ->  ether to c\n",
      "53 ether to c  ->  ther to co\n",
      "54 ther to co  ->  her to col\n",
      "55 her to col  ->  er to coll\n",
      "56 er to coll  ->  r to colle\n",
      "57 r to colle  ->   to collec\n",
      "58  to collec  ->  to collect\n",
      "59 to collect  ->  o collect \n",
      "60 o collect   ->   collect w\n",
      "61  collect w  ->  collect wo\n",
      "62 collect wo  ->  ollect woo\n",
      "63 ollect woo  ->  llect wood\n",
      "64 llect wood  ->  lect wood \n",
      "65 lect wood   ->  ect wood a\n",
      "66 ect wood a  ->  ct wood an\n",
      "67 ct wood an  ->  t wood and\n",
      "68 t wood and  ->   wood and \n",
      "69  wood and   ->  wood and d\n",
      "70 wood and d  ->  ood and do\n",
      "71 ood and do  ->  od and don\n",
      "72 od and don  ->  d and don'\n",
      "73 d and don'  ->   and don't\n",
      "74  and don't  ->  and don't \n",
      "75 and don't   ->  nd don't a\n",
      "76 nd don't a  ->  d don't as\n",
      "77 d don't as  ->   don't ass\n",
      "78  don't ass  ->  don't assi\n",
      "79 don't assi  ->  on't assig\n",
      "80 on't assig  ->  n't assign\n",
      "81 n't assign  ->  't assign \n",
      "82 't assign   ->  t assign t\n",
      "83 t assign t  ->   assign th\n",
      "84  assign th  ->  assign the\n",
      "85 assign the  ->  ssign them\n",
      "86 ssign them  ->  sign them \n",
      "87 sign them   ->  ign them t\n",
      "88 ign them t  ->  gn them ta\n",
      "89 gn them ta  ->  n them tas\n",
      "90 n them tas  ->   them task\n",
      "91  them task  ->  them tasks\n",
      "92 them tasks  ->  hem tasks \n",
      "93 hem tasks   ->  em tasks a\n",
      "94 em tasks a  ->  m tasks an\n",
      "95 m tasks an  ->   tasks and\n",
      "96  tasks and  ->  tasks and \n",
      "97 tasks and   ->  asks and w\n",
      "98 asks and w  ->  sks and wo\n",
      "99 sks and wo  ->  ks and wor\n",
      "100 ks and wor  ->  s and work\n",
      "101 s and work  ->   and work,\n",
      "102  and work,  ->  and work, \n",
      "103 and work,   ->  nd work, b\n",
      "104 nd work, b  ->  d work, bu\n",
      "105 d work, bu  ->   work, but\n",
      "106  work, but  ->  work, but \n",
      "107 work, but   ->  ork, but r\n",
      "108 ork, but r  ->  rk, but ra\n",
      "109 rk, but ra  ->  k, but rat\n",
      "110 k, but rat  ->  , but rath\n",
      "111 , but rath  ->   but rathe\n",
      "112  but rathe  ->  but rather\n",
      "113 but rather  ->  ut rather \n",
      "114 ut rather   ->  t rather t\n",
      "115 t rather t  ->   rather te\n",
      "116  rather te  ->  rather tea\n",
      "117 rather tea  ->  ather teac\n",
      "118 ather teac  ->  ther teach\n",
      "119 ther teach  ->  her teach \n",
      "120 her teach   ->  er teach t\n",
      "121 er teach t  ->  r teach th\n",
      "122 r teach th  ->   teach the\n",
      "123  teach the  ->  teach them\n",
      "124 teach them  ->  each them \n",
      "125 each them   ->  ach them t\n",
      "126 ach them t  ->  ch them to\n",
      "127 ch them to  ->  h them to \n",
      "128 h them to   ->   them to l\n",
      "129  them to l  ->  them to lo\n",
      "130 them to lo  ->  hem to lon\n",
      "131 hem to lon  ->  em to long\n",
      "132 em to long  ->  m to long \n",
      "133 m to long   ->   to long f\n",
      "134  to long f  ->  to long fo\n",
      "135 to long fo  ->  o long for\n",
      "136 o long for  ->   long for \n",
      "137  long for   ->  long for t\n",
      "138 long for t  ->  ong for th\n",
      "139 ong for th  ->  ng for the\n",
      "140 ng for the  ->  g for the \n",
      "141 g for the   ->   for the e\n",
      "142  for the e  ->  for the en\n",
      "143 for the en  ->  or the end\n",
      "144 or the end  ->  r the endl\n",
      "145 r the endl  ->   the endle\n",
      "146  the endle  ->  the endles\n",
      "147 the endles  ->  he endless\n",
      "148 he endless  ->  e endless \n",
      "149 e endless   ->   endless i\n",
      "150  endless i  ->  endless im\n",
      "151 endless im  ->  ndless imm\n",
      "152 ndless imm  ->  dless imme\n",
      "153 dless imme  ->  less immen\n",
      "154 less immen  ->  ess immens\n",
      "155 ess immens  ->  ss immensi\n",
      "156 ss immensi  ->  s immensit\n",
      "157 s immensit  ->   immensity\n",
      "158  immensity  ->  immensity \n",
      "159 immensity   ->  mmensity o\n",
      "160 mmensity o  ->  mensity of\n",
      "161 mensity of  ->  ensity of \n",
      "162 ensity of   ->  nsity of t\n",
      "163 nsity of t  ->  sity of th\n",
      "164 sity of th  ->  ity of the\n",
      "165 ity of the  ->  ty of the \n",
      "166 ty of the   ->  y of the s\n",
      "167 y of the s  ->   of the se\n",
      "168  of the se  ->  of the sea\n",
      "169 of the sea  ->  f the sea.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1 : i + sequence_length + 1]\n",
    "    print(i, x_str, ' -> ', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])\n",
    "    y_data.append([char_dic[c] for c in y_str])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13, 5, 2, 14, 20, 7, 2, 10, 8, 11], [5, 2, 14, 20, 7, 2, 10, 8, 11, 18], [2, 14, 20, 7, 2, 10, 8, 11, 18, 2], [14, 20, 7, 2, 10, 8, 11, 18, 2, 18], [20, 7, 2, 10, 8, 11, 18, 2, 18, 20], [7, 2, 10, 8, 11, 18, 2, 18, 20, 2], [2, 10, 8, 11, 18, 2, 18, 20, 2, 12], [10, 8, 11, 18, 2, 18, 20, 2, 12, 7], [8, 11, 18, 2, 18, 20, 2, 12, 7, 13], [11, 18, 2, 18, 20, 2, 12, 7, 13, 3], [18, 2, 18, 20, 2, 12, 7, 13, 3, 19], [2, 18, 20, 2, 12, 7, 13, 3, 19, 2], [18, 20, 2, 12, 7, 13, 3, 19, 2, 8], [20, 2, 12, 7, 13, 3, 19, 2, 8, 2], [2, 12, 7, 13, 3, 19, 2, 8, 2, 1], [12, 7, 13, 3, 19, 2, 8, 2, 1, 22], [7, 13, 3, 19, 2, 8, 2, 1, 22, 13], [13, 3, 19, 2, 8, 2, 1, 22, 13, 16], [3, 19, 2, 8, 2, 1, 22, 13, 16, 4], [19, 2, 8, 2, 1, 22, 13, 16, 4, 2], [2, 8, 2, 1, 22, 13, 16, 4, 2, 19], [8, 2, 1, 22, 13, 16, 4, 2, 19, 20], [2, 1, 22, 13, 16, 4, 2, 19, 20, 11], [1, 22, 13, 16, 4, 2, 19, 20, 11, 15], [22, 13, 16, 4, 2, 19, 20, 11, 15, 18], [13, 16, 4, 2, 19, 20, 11, 15, 18, 2], [16, 4, 2, 19, 20, 11, 15, 18, 2, 19], [4, 2, 19, 20, 11, 15, 18, 2, 19, 21], [2, 19, 20, 11, 15, 18, 2, 19, 21, 7], [19, 20, 11, 15, 18, 2, 19, 21, 7, 24], [20, 11, 15, 18, 2, 19, 21, 7, 24, 2], [11, 15, 18, 2, 19, 21, 7, 24, 2, 7], [15, 18, 2, 19, 21, 7, 24, 2, 7, 16], [18, 2, 19, 21, 7, 24, 2, 7, 16, 2], [2, 19, 21, 7, 24, 2, 7, 16, 2, 16], [19, 21, 7, 24, 2, 7, 16, 2, 16, 0], [21, 7, 24, 2, 7, 16, 2, 16, 0, 20], [7, 24, 2, 7, 16, 2, 16, 0, 20, 16], [24, 2, 7, 16, 2, 16, 0, 20, 16, 3], [2, 7, 16, 2, 16, 0, 20, 16, 3, 0], [7, 16, 2, 16, 0, 20, 16, 3, 0, 2], [16, 2, 16, 0, 20, 16, 3, 0, 2, 18], [2, 16, 0, 20, 16, 3, 0, 2, 18, 20], [16, 0, 20, 16, 3, 0, 2, 18, 20, 6], [0, 20, 16, 3, 0, 2, 18, 20, 6, 0], [20, 16, 3, 0, 2, 18, 20, 6, 0, 18], [16, 3, 0, 2, 18, 20, 6, 0, 18, 22], [3, 0, 2, 18, 20, 6, 0, 18, 22, 0], [0, 2, 18, 20, 6, 0, 18, 22, 0, 21], [2, 18, 20, 6, 0, 18, 22, 0, 21, 2], [18, 20, 6, 0, 18, 22, 0, 21, 2, 18], [20, 6, 0, 18, 22, 0, 21, 2, 18, 20], [6, 0, 18, 22, 0, 21, 2, 18, 20, 2], [0, 18, 22, 0, 21, 2, 18, 20, 2, 17], [18, 22, 0, 21, 2, 18, 20, 2, 17, 20], [22, 0, 21, 2, 18, 20, 2, 17, 20, 3], [0, 21, 2, 18, 20, 2, 17, 20, 3, 3], [21, 2, 18, 20, 2, 17, 20, 3, 3, 0], [2, 18, 20, 2, 17, 20, 3, 3, 0, 17], [18, 20, 2, 17, 20, 3, 3, 0, 17, 18], [20, 2, 17, 20, 3, 3, 0, 17, 18, 2], [2, 17, 20, 3, 3, 0, 17, 18, 2, 10], [17, 20, 3, 3, 0, 17, 18, 2, 10, 20], [20, 3, 3, 0, 17, 18, 2, 10, 20, 20], [3, 3, 0, 17, 18, 2, 10, 20, 20, 19], [3, 0, 17, 18, 2, 10, 20, 20, 19, 2], [0, 17, 18, 2, 10, 20, 20, 19, 2, 8], [17, 18, 2, 10, 20, 20, 19, 2, 8, 11], [18, 2, 10, 20, 20, 19, 2, 8, 11, 19], [2, 10, 20, 20, 19, 2, 8, 11, 19, 2], [10, 20, 20, 19, 2, 8, 11, 19, 2, 19], [20, 20, 19, 2, 8, 11, 19, 2, 19, 20], [20, 19, 2, 8, 11, 19, 2, 19, 20, 11], [19, 2, 8, 11, 19, 2, 19, 20, 11, 15], [2, 8, 11, 19, 2, 19, 20, 11, 15, 18], [8, 11, 19, 2, 19, 20, 11, 15, 18, 2], [11, 19, 2, 19, 20, 11, 15, 18, 2, 8], [19, 2, 19, 20, 11, 15, 18, 2, 8, 1], [2, 19, 20, 11, 15, 18, 2, 8, 1, 1], [19, 20, 11, 15, 18, 2, 8, 1, 1, 13], [20, 11, 15, 18, 2, 8, 1, 1, 13, 6], [11, 15, 18, 2, 8, 1, 1, 13, 6, 11], [15, 18, 2, 8, 1, 1, 13, 6, 11, 2], [18, 2, 8, 1, 1, 13, 6, 11, 2, 18], [2, 8, 1, 1, 13, 6, 11, 2, 18, 22], [8, 1, 1, 13, 6, 11, 2, 18, 22, 0], [1, 1, 13, 6, 11, 2, 18, 22, 0, 24], [1, 13, 6, 11, 2, 18, 22, 0, 24, 2], [13, 6, 11, 2, 18, 22, 0, 24, 2, 18], [6, 11, 2, 18, 22, 0, 24, 2, 18, 8], [11, 2, 18, 22, 0, 24, 2, 18, 8, 1], [2, 18, 22, 0, 24, 2, 18, 8, 1, 9], [18, 22, 0, 24, 2, 18, 8, 1, 9, 1], [22, 0, 24, 2, 18, 8, 1, 9, 1, 2], [0, 24, 2, 18, 8, 1, 9, 1, 2, 8], [24, 2, 18, 8, 1, 9, 1, 2, 8, 11], [2, 18, 8, 1, 9, 1, 2, 8, 11, 19], [18, 8, 1, 9, 1, 2, 8, 11, 19, 2], [8, 1, 9, 1, 2, 8, 11, 19, 2, 10], [1, 9, 1, 2, 8, 11, 19, 2, 10, 20], [9, 1, 2, 8, 11, 19, 2, 10, 20, 21], [1, 2, 8, 11, 19, 2, 10, 20, 21, 9], [2, 8, 11, 19, 2, 10, 20, 21, 9, 4], [8, 11, 19, 2, 10, 20, 21, 9, 4, 2], [11, 19, 2, 10, 20, 21, 9, 4, 2, 12], [19, 2, 10, 20, 21, 9, 4, 2, 12, 7], [2, 10, 20, 21, 9, 4, 2, 12, 7, 18], [10, 20, 21, 9, 4, 2, 12, 7, 18, 2], [20, 21, 9, 4, 2, 12, 7, 18, 2, 21], [21, 9, 4, 2, 12, 7, 18, 2, 21, 8], [9, 4, 2, 12, 7, 18, 2, 21, 8, 18], [4, 2, 12, 7, 18, 2, 21, 8, 18, 22], [2, 12, 7, 18, 2, 21, 8, 18, 22, 0], [12, 7, 18, 2, 21, 8, 18, 22, 0, 21], [7, 18, 2, 21, 8, 18, 22, 0, 21, 2], [18, 2, 21, 8, 18, 22, 0, 21, 2, 18], [2, 21, 8, 18, 22, 0, 21, 2, 18, 0], [21, 8, 18, 22, 0, 21, 2, 18, 0, 8], [8, 18, 22, 0, 21, 2, 18, 0, 8, 17], [18, 22, 0, 21, 2, 18, 0, 8, 17, 22], [22, 0, 21, 2, 18, 0, 8, 17, 22, 2], [0, 21, 2, 18, 0, 8, 17, 22, 2, 18], [21, 2, 18, 0, 8, 17, 22, 2, 18, 22], [2, 18, 0, 8, 17, 22, 2, 18, 22, 0], [18, 0, 8, 17, 22, 2, 18, 22, 0, 24], [0, 8, 17, 22, 2, 18, 22, 0, 24, 2], [8, 17, 22, 2, 18, 22, 0, 24, 2, 18], [17, 22, 2, 18, 22, 0, 24, 2, 18, 20], [22, 2, 18, 22, 0, 24, 2, 18, 20, 2], [2, 18, 22, 0, 24, 2, 18, 20, 2, 3], [18, 22, 0, 24, 2, 18, 20, 2, 3, 20], [22, 0, 24, 2, 18, 20, 2, 3, 20, 11], [0, 24, 2, 18, 20, 2, 3, 20, 11, 6], [24, 2, 18, 20, 2, 3, 20, 11, 6, 2], [2, 18, 20, 2, 3, 20, 11, 6, 2, 5], [18, 20, 2, 3, 20, 11, 6, 2, 5, 20], [20, 2, 3, 20, 11, 6, 2, 5, 20, 21], [2, 3, 20, 11, 6, 2, 5, 20, 21, 2], [3, 20, 11, 6, 2, 5, 20, 21, 2, 18], [20, 11, 6, 2, 5, 20, 21, 2, 18, 22], [11, 6, 2, 5, 20, 21, 2, 18, 22, 0], [6, 2, 5, 20, 21, 2, 18, 22, 0, 2], [2, 5, 20, 21, 2, 18, 22, 0, 2, 0], [5, 20, 21, 2, 18, 22, 0, 2, 0, 11], [20, 21, 2, 18, 22, 0, 2, 0, 11, 19], [21, 2, 18, 22, 0, 2, 0, 11, 19, 3], [2, 18, 22, 0, 2, 0, 11, 19, 3, 0], [18, 22, 0, 2, 0, 11, 19, 3, 0, 1], [22, 0, 2, 0, 11, 19, 3, 0, 1, 1], [0, 2, 0, 11, 19, 3, 0, 1, 1, 2], [2, 0, 11, 19, 3, 0, 1, 1, 2, 13], [0, 11, 19, 3, 0, 1, 1, 2, 13, 24], [11, 19, 3, 0, 1, 1, 2, 13, 24, 24], [19, 3, 0, 1, 1, 2, 13, 24, 24, 0], [3, 0, 1, 1, 2, 13, 24, 24, 0, 11], [0, 1, 1, 2, 13, 24, 24, 0, 11, 1], [1, 1, 2, 13, 24, 24, 0, 11, 1, 13], [1, 2, 13, 24, 24, 0, 11, 1, 13, 18], [2, 13, 24, 24, 0, 11, 1, 13, 18, 14], [13, 24, 24, 0, 11, 1, 13, 18, 14, 2], [24, 24, 0, 11, 1, 13, 18, 14, 2, 20], [24, 0, 11, 1, 13, 18, 14, 2, 20, 5], [0, 11, 1, 13, 18, 14, 2, 20, 5, 2], [11, 1, 13, 18, 14, 2, 20, 5, 2, 18], [1, 13, 18, 14, 2, 20, 5, 2, 18, 22], [13, 18, 14, 2, 20, 5, 2, 18, 22, 0], [18, 14, 2, 20, 5, 2, 18, 22, 0, 2], [14, 2, 20, 5, 2, 18, 22, 0, 2, 1], [2, 20, 5, 2, 18, 22, 0, 2, 1, 0], [20, 5, 2, 18, 22, 0, 2, 1, 0, 8]]\n",
      "[[5, 2, 14, 20, 7, 2, 10, 8, 11, 18], [2, 14, 20, 7, 2, 10, 8, 11, 18, 2], [14, 20, 7, 2, 10, 8, 11, 18, 2, 18], [20, 7, 2, 10, 8, 11, 18, 2, 18, 20], [7, 2, 10, 8, 11, 18, 2, 18, 20, 2], [2, 10, 8, 11, 18, 2, 18, 20, 2, 12], [10, 8, 11, 18, 2, 18, 20, 2, 12, 7], [8, 11, 18, 2, 18, 20, 2, 12, 7, 13], [11, 18, 2, 18, 20, 2, 12, 7, 13, 3], [18, 2, 18, 20, 2, 12, 7, 13, 3, 19], [2, 18, 20, 2, 12, 7, 13, 3, 19, 2], [18, 20, 2, 12, 7, 13, 3, 19, 2, 8], [20, 2, 12, 7, 13, 3, 19, 2, 8, 2], [2, 12, 7, 13, 3, 19, 2, 8, 2, 1], [12, 7, 13, 3, 19, 2, 8, 2, 1, 22], [7, 13, 3, 19, 2, 8, 2, 1, 22, 13], [13, 3, 19, 2, 8, 2, 1, 22, 13, 16], [3, 19, 2, 8, 2, 1, 22, 13, 16, 4], [19, 2, 8, 2, 1, 22, 13, 16, 4, 2], [2, 8, 2, 1, 22, 13, 16, 4, 2, 19], [8, 2, 1, 22, 13, 16, 4, 2, 19, 20], [2, 1, 22, 13, 16, 4, 2, 19, 20, 11], [1, 22, 13, 16, 4, 2, 19, 20, 11, 15], [22, 13, 16, 4, 2, 19, 20, 11, 15, 18], [13, 16, 4, 2, 19, 20, 11, 15, 18, 2], [16, 4, 2, 19, 20, 11, 15, 18, 2, 19], [4, 2, 19, 20, 11, 15, 18, 2, 19, 21], [2, 19, 20, 11, 15, 18, 2, 19, 21, 7], [19, 20, 11, 15, 18, 2, 19, 21, 7, 24], [20, 11, 15, 18, 2, 19, 21, 7, 24, 2], [11, 15, 18, 2, 19, 21, 7, 24, 2, 7], [15, 18, 2, 19, 21, 7, 24, 2, 7, 16], [18, 2, 19, 21, 7, 24, 2, 7, 16, 2], [2, 19, 21, 7, 24, 2, 7, 16, 2, 16], [19, 21, 7, 24, 2, 7, 16, 2, 16, 0], [21, 7, 24, 2, 7, 16, 2, 16, 0, 20], [7, 24, 2, 7, 16, 2, 16, 0, 20, 16], [24, 2, 7, 16, 2, 16, 0, 20, 16, 3], [2, 7, 16, 2, 16, 0, 20, 16, 3, 0], [7, 16, 2, 16, 0, 20, 16, 3, 0, 2], [16, 2, 16, 0, 20, 16, 3, 0, 2, 18], [2, 16, 0, 20, 16, 3, 0, 2, 18, 20], [16, 0, 20, 16, 3, 0, 2, 18, 20, 6], [0, 20, 16, 3, 0, 2, 18, 20, 6, 0], [20, 16, 3, 0, 2, 18, 20, 6, 0, 18], [16, 3, 0, 2, 18, 20, 6, 0, 18, 22], [3, 0, 2, 18, 20, 6, 0, 18, 22, 0], [0, 2, 18, 20, 6, 0, 18, 22, 0, 21], [2, 18, 20, 6, 0, 18, 22, 0, 21, 2], [18, 20, 6, 0, 18, 22, 0, 21, 2, 18], [20, 6, 0, 18, 22, 0, 21, 2, 18, 20], [6, 0, 18, 22, 0, 21, 2, 18, 20, 2], [0, 18, 22, 0, 21, 2, 18, 20, 2, 17], [18, 22, 0, 21, 2, 18, 20, 2, 17, 20], [22, 0, 21, 2, 18, 20, 2, 17, 20, 3], [0, 21, 2, 18, 20, 2, 17, 20, 3, 3], [21, 2, 18, 20, 2, 17, 20, 3, 3, 0], [2, 18, 20, 2, 17, 20, 3, 3, 0, 17], [18, 20, 2, 17, 20, 3, 3, 0, 17, 18], [20, 2, 17, 20, 3, 3, 0, 17, 18, 2], [2, 17, 20, 3, 3, 0, 17, 18, 2, 10], [17, 20, 3, 3, 0, 17, 18, 2, 10, 20], [20, 3, 3, 0, 17, 18, 2, 10, 20, 20], [3, 3, 0, 17, 18, 2, 10, 20, 20, 19], [3, 0, 17, 18, 2, 10, 20, 20, 19, 2], [0, 17, 18, 2, 10, 20, 20, 19, 2, 8], [17, 18, 2, 10, 20, 20, 19, 2, 8, 11], [18, 2, 10, 20, 20, 19, 2, 8, 11, 19], [2, 10, 20, 20, 19, 2, 8, 11, 19, 2], [10, 20, 20, 19, 2, 8, 11, 19, 2, 19], [20, 20, 19, 2, 8, 11, 19, 2, 19, 20], [20, 19, 2, 8, 11, 19, 2, 19, 20, 11], [19, 2, 8, 11, 19, 2, 19, 20, 11, 15], [2, 8, 11, 19, 2, 19, 20, 11, 15, 18], [8, 11, 19, 2, 19, 20, 11, 15, 18, 2], [11, 19, 2, 19, 20, 11, 15, 18, 2, 8], [19, 2, 19, 20, 11, 15, 18, 2, 8, 1], [2, 19, 20, 11, 15, 18, 2, 8, 1, 1], [19, 20, 11, 15, 18, 2, 8, 1, 1, 13], [20, 11, 15, 18, 2, 8, 1, 1, 13, 6], [11, 15, 18, 2, 8, 1, 1, 13, 6, 11], [15, 18, 2, 8, 1, 1, 13, 6, 11, 2], [18, 2, 8, 1, 1, 13, 6, 11, 2, 18], [2, 8, 1, 1, 13, 6, 11, 2, 18, 22], [8, 1, 1, 13, 6, 11, 2, 18, 22, 0], [1, 1, 13, 6, 11, 2, 18, 22, 0, 24], [1, 13, 6, 11, 2, 18, 22, 0, 24, 2], [13, 6, 11, 2, 18, 22, 0, 24, 2, 18], [6, 11, 2, 18, 22, 0, 24, 2, 18, 8], [11, 2, 18, 22, 0, 24, 2, 18, 8, 1], [2, 18, 22, 0, 24, 2, 18, 8, 1, 9], [18, 22, 0, 24, 2, 18, 8, 1, 9, 1], [22, 0, 24, 2, 18, 8, 1, 9, 1, 2], [0, 24, 2, 18, 8, 1, 9, 1, 2, 8], [24, 2, 18, 8, 1, 9, 1, 2, 8, 11], [2, 18, 8, 1, 9, 1, 2, 8, 11, 19], [18, 8, 1, 9, 1, 2, 8, 11, 19, 2], [8, 1, 9, 1, 2, 8, 11, 19, 2, 10], [1, 9, 1, 2, 8, 11, 19, 2, 10, 20], [9, 1, 2, 8, 11, 19, 2, 10, 20, 21], [1, 2, 8, 11, 19, 2, 10, 20, 21, 9], [2, 8, 11, 19, 2, 10, 20, 21, 9, 4], [8, 11, 19, 2, 10, 20, 21, 9, 4, 2], [11, 19, 2, 10, 20, 21, 9, 4, 2, 12], [19, 2, 10, 20, 21, 9, 4, 2, 12, 7], [2, 10, 20, 21, 9, 4, 2, 12, 7, 18], [10, 20, 21, 9, 4, 2, 12, 7, 18, 2], [20, 21, 9, 4, 2, 12, 7, 18, 2, 21], [21, 9, 4, 2, 12, 7, 18, 2, 21, 8], [9, 4, 2, 12, 7, 18, 2, 21, 8, 18], [4, 2, 12, 7, 18, 2, 21, 8, 18, 22], [2, 12, 7, 18, 2, 21, 8, 18, 22, 0], [12, 7, 18, 2, 21, 8, 18, 22, 0, 21], [7, 18, 2, 21, 8, 18, 22, 0, 21, 2], [18, 2, 21, 8, 18, 22, 0, 21, 2, 18], [2, 21, 8, 18, 22, 0, 21, 2, 18, 0], [21, 8, 18, 22, 0, 21, 2, 18, 0, 8], [8, 18, 22, 0, 21, 2, 18, 0, 8, 17], [18, 22, 0, 21, 2, 18, 0, 8, 17, 22], [22, 0, 21, 2, 18, 0, 8, 17, 22, 2], [0, 21, 2, 18, 0, 8, 17, 22, 2, 18], [21, 2, 18, 0, 8, 17, 22, 2, 18, 22], [2, 18, 0, 8, 17, 22, 2, 18, 22, 0], [18, 0, 8, 17, 22, 2, 18, 22, 0, 24], [0, 8, 17, 22, 2, 18, 22, 0, 24, 2], [8, 17, 22, 2, 18, 22, 0, 24, 2, 18], [17, 22, 2, 18, 22, 0, 24, 2, 18, 20], [22, 2, 18, 22, 0, 24, 2, 18, 20, 2], [2, 18, 22, 0, 24, 2, 18, 20, 2, 3], [18, 22, 0, 24, 2, 18, 20, 2, 3, 20], [22, 0, 24, 2, 18, 20, 2, 3, 20, 11], [0, 24, 2, 18, 20, 2, 3, 20, 11, 6], [24, 2, 18, 20, 2, 3, 20, 11, 6, 2], [2, 18, 20, 2, 3, 20, 11, 6, 2, 5], [18, 20, 2, 3, 20, 11, 6, 2, 5, 20], [20, 2, 3, 20, 11, 6, 2, 5, 20, 21], [2, 3, 20, 11, 6, 2, 5, 20, 21, 2], [3, 20, 11, 6, 2, 5, 20, 21, 2, 18], [20, 11, 6, 2, 5, 20, 21, 2, 18, 22], [11, 6, 2, 5, 20, 21, 2, 18, 22, 0], [6, 2, 5, 20, 21, 2, 18, 22, 0, 2], [2, 5, 20, 21, 2, 18, 22, 0, 2, 0], [5, 20, 21, 2, 18, 22, 0, 2, 0, 11], [20, 21, 2, 18, 22, 0, 2, 0, 11, 19], [21, 2, 18, 22, 0, 2, 0, 11, 19, 3], [2, 18, 22, 0, 2, 0, 11, 19, 3, 0], [18, 22, 0, 2, 0, 11, 19, 3, 0, 1], [22, 0, 2, 0, 11, 19, 3, 0, 1, 1], [0, 2, 0, 11, 19, 3, 0, 1, 1, 2], [2, 0, 11, 19, 3, 0, 1, 1, 2, 13], [0, 11, 19, 3, 0, 1, 1, 2, 13, 24], [11, 19, 3, 0, 1, 1, 2, 13, 24, 24], [19, 3, 0, 1, 1, 2, 13, 24, 24, 0], [3, 0, 1, 1, 2, 13, 24, 24, 0, 11], [0, 1, 1, 2, 13, 24, 24, 0, 11, 1], [1, 1, 2, 13, 24, 24, 0, 11, 1, 13], [1, 2, 13, 24, 24, 0, 11, 1, 13, 18], [2, 13, 24, 24, 0, 11, 1, 13, 18, 14], [13, 24, 24, 0, 11, 1, 13, 18, 14, 2], [24, 24, 0, 11, 1, 13, 18, 14, 2, 20], [24, 0, 11, 1, 13, 18, 14, 2, 20, 5], [0, 11, 1, 13, 18, 14, 2, 20, 5, 2], [11, 1, 13, 18, 14, 2, 20, 5, 2, 18], [1, 13, 18, 14, 2, 20, 5, 2, 18, 22], [13, 18, 14, 2, 20, 5, 2, 18, 22, 0], [18, 14, 2, 20, 5, 2, 18, 22, 0, 2], [14, 2, 20, 5, 2, 18, 22, 0, 2, 1], [2, 20, 5, 2, 18, 22, 0, 2, 1, 0], [20, 5, 2, 18, 22, 0, 2, 1, 0, 8], [5, 2, 18, 22, 0, 2, 1, 0, 8, 23]]\n"
     ]
    }
   ],
   "source": [
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  2, 14, 20,  7,  2, 10,  8, 11, 18])\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dic_size, hidden_size, 2) # 층을 두 개 쌓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1700, 25])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, dic_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10])\n",
      "torch.Size([1700])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape) # 정확도 측정 시에는 (170, 10) -> (1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragaaaagralagaaaaaaaagagaakggaagaaaaraakakagaagaaagaaagaaraaaagagaaloarakaagraaaagaaaaggaagagaaaaargggaagaagaagagaaaaagaaaaaaaggaagaaaaakagagaaagaaaaakagaaaggaaaaaggaaaagaagaakgaa\n",
      "                                                                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                   \n",
      "tlyonp.oipypppppoybpblspypyp.p.pyaypypppplppayl.pbpppyllp.plppylppyllpppppp.ppbyypyaylsppallplyplyaylpplsppoopyaypspyasp.aypypyylppypppylplpll.pppppbpsp.p.l.l.pyp.lylyplyyplpp.ppp\n",
      "teeosashoteeeeeoheteeheoseeoeoesheeoeoeeoeeteeieoeeoeeeeteseeeeeeeeseeoeeoteteeeeeeaeheaeeeseeeeeeeeteeheaeeeteaeheaseeteoeeeeeteeeeoeeeeeseeleoeeseeheoheeoeaeoeleteaeeoheeheaeeoe\n",
      "t eoeototototee otooeototoeotototoeotoeetoeootoeotoooeteototeetestee eoeototsteototoeoeototoeeoetooeseeotoeototoeoeototoeoeototsetoeoeteoeotsseoeteotototoeoeoeoeototseootetoeototo\n",
      "t toe e t t oteogt oe e e e e e t t e edt t ee t eot eee e ttete t ete e e oett e t e e t e oe ee egttt e e e t e e e t e t t t tt e eee et e toeee e e e e e e e e t t  oee e e e \n",
      " ot t o o  eoe o  t o oo    uoot t o o wo  o 't o oeoooot o e'o oo t oooootrt o oo to  e te  et   o t et ot     t t 'oo  e t e t oo o uoeao o  o  o t uo'o o t'oo  to  e o a r tott\n",
      "t     o o  to to    o oo    too t to o eo to t t  o o     o  to      oooootrt o  ot     t t  o tt      ot to  t e t d o t  t  t   o o oo to t to  o   tot  ot to    t    o a e t   \n",
      "t  o twhn   o tot   t n t   e n t to    o to t   to      o   to t o   o    n  to  t     t   o  tt  t      ton t t   t   t t o t   o    n to t toekt   t t t t t   o      nnw   t   \n",
      "t  o lwont to eo    thdh h  ehntt   n  to tohet heo  t    tn to   n  to   tn  to t h    t  tn h to t      tontd t   ehsh h tn t  to h to ton   o htht whd eoe th  n  t  onndh  eh t\n",
      "t ao  wont ao to  totothe   tont   on  to aohet hto       t  to  tn  to   tn  to t,h on     n   to    tos don , to oths eh t     eo   tn ton  to  to  tot eos th an     on to  tod \n",
      "l eo  wont to ami totothe t tont  eo t to a h em to   oe  t  lo  am  eon  tn  to t, eos   e n l to  t todotone, lht tht e  t     eoe ean lont eo  to  tht eos t  ent l eon to  thtt\n",
      "l eo  wont t  auill todhilt tosl, e  p tmep o em to ei  l t  lo lemt eon etnt eonl, eos   eaosl w s t ehd tonp, l t tht e st   t l e  to lont lo  toe thd e sli  end i eon to  thdt\n",
      "' eo  tont to tusld tnd it, ton', i sp toetonndc to es e  t  wo  emt eon etns eon', ens   itoe  toe t ind eonp, tmt e t ep t  cseloe  tonwont to  toe thd e slt  ensle eto toe th l\n",
      "' eon tong to 'usie tod it, ton', torc toetonule to  s e  to wo demt eon  tnsleon't eosit ttoe  toet  tnd eon , wme e t e  to tsetoe  tonwent ton toe tnd e s t  ensle eto toe th '\n",
      "' ton tooi to 'nsie tot it, ton't toum toe'enuie to  t e  to to demt toog tnd ton't tnsit ttoe  toet  tnd tonk, tmt t t e  to th toe  to went ton toe tnd ens tn enslt eto toe th '\n",
      "t ton tooi eo tus e tot i', ton't toum to 'eou e to  the  to to demt toog tnd ton't tos t stoe  toett tnt took, tmt t the  te th toem to wont to  toe tnd ensldm enslt eoo toemth t\n",
      "t ton wooi to bus e tnt il, don't doum to teou e to eth r to aoidemt doog tnd won't ans t  toe  toess dnt dook, dmt trth   te cs toem to wout to  toe tnd ens dm enild  oo toe th t\n",
      "t ton wont to bns d anthip, don't ao m ao reon e to eth r th aoidemt dooi dnd don't ans t  toe  tos s dnd donk, dmt amth r th c, toem to bout ton the tnd ers dm enslg  on therdh t\n",
      "t aon wont tonbnsld andhip, don't ao m tn lehnle th eth r th co lemt doni and don't anslt  toe  tosts dnd donk, bmt arth r th th toem to lont ton therdnd ers dm onslg  on therdh t\n",
      "t aon bont to built andhip, don't aonm tn peoule to ether th co leut dons and won't anslt  toem tosts bnd wonk, but arther th th toem to lont ton therdnd ers tm enslt  on therdh t\n",
      "t aon lost to wuilt andhip, don't aoum tu peoule to  ther to co leut wons and won't anslt  toem tosts bnd wonk, but arther th th toem to lont ton the dnd ers tm enilty of thesemit\n",
      "p aon lon' to build andhip, don't aoum tu peosle to  ther to co leut wond and won't ansit  toem tosts bnd wonk, but arther to ch toem to long bon the dnd ers tm ensity of the dhit\n",
      "p ton lon' ao build andhip, don't doum tu people to  ther to co lect wond and don't dnsit  toem tosms tnd donk, but arthe  to ch them to long bon the dnd ers tm ensity of the dhit\n",
      "p ton wons to build andhip, don't doum tp people to ether to co lect wond and won't dnsitn toem tosks tnd wonk, but aathe  to ch them to long ton the dndlers tm ensity of the dmit\n",
      "p tor wons to build andhip, don't aoum tp people to ether te co lect wond and won't ansitn toem tosks and wonk, but aather te ch them to long for the dndlers tm ensity of teemd it\n",
      "p tormwant to build anthip, don't aoum up people to ether te co lect wond and don't ansign toem tosks and donk, but aather te ch them to long for the dnd ens im ensity of themd it\n",
      "p tor want to build anthip, don't doum up people to ether to co lect wond and don't ansign toem tosks dnd dork, but aather torch them to long for the dnd ens im ensity of thema it\n",
      "p tor want to build aathip, don't doum up people to ether to co lect wood and don't ansign toem tosks dnd dork, but aather torch ther to long for therdnd ens im ensity of thema it\n",
      "p tor want to build aathip, don't doum up people to ether to collect wood and don't ansign toem tosks and work, but tather toach them to long for therdnd ens im ensity of thema sp\n",
      "p tor want to build anship, don't doum up people together to collect wood and don't ansign toem tosks and work, but tather toach them to long for the dnd ens immensity of the a sp\n",
      "p aor want to build anship, don't aoum up people together te collect wood and don't ansign toem tosks and work, but aather teach them to long for the dnd ens immensigy of the a sm\n",
      "t dor want to build asship, don't aoum up people thgether te collect wood and don't ansign toem tasks and work, but rather teach them to long for the end ess immensigy of the a sm\n",
      "t dor want to build asship, don't drum up people thgether to collect wood and don't ansign toem tosks and work, but rather toach them to long for the andless immensity of the a am\n",
      "t dor want to build asship, don't drum up people together to collect wood and don't ansign them tasks and work, but rather toach them to long for the endless immensity of the e am\n",
      "t dor want to build asship, don't drum up people together to collect wood asd don't ansign them tosks and work, but rather toach them to long for the endless immensity of the e am\n",
      "t yor want to build asship, don't drum up people together to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the e sm\n",
      "p yor want to build asship, don't drum up people together to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the e sm\n",
      "p yor want to build asship, don't drum up people together to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the sham\n",
      "p you want to build asship, don't drum up people together to collect wood and don't dnsign them tasks and work, but rather toach them to long for the endless immensity of the e at\n",
      "t you want to build asship, don't arum up people together to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the s am\n",
      "t dou want to build asship, don't drum up people together to collect wood and don't dnsign them tasks and work, but rather toach them to long for the endless immensity of the s at\n",
      "t rou want to build asship, don't arum up people together to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the e am\n",
      "t dou want to build asship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the sndless immensity of the s ap\n",
      "p rou want to build asship, don't drum up people together to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the e ap\n",
      "p dou want to build asship, don't drum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the s ap\n",
      "p you want to build asship, don't drum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the ehap\n",
      "p you want to build asship, don't drum up people together to collect wood and don't dssign them tosks and dork, but rather toach them to long for the endless immensity of the shap\n",
      "p you want to build asship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the eesp\n",
      "p you want to butld asship, don't drum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the sndless immensity of the s ap\n",
      "p you want to build asship, don't drum up people together to collect wood and don't assign them tosks and dork, but rather teath them ta long for the endless immenhity of the ehap\n",
      "p you want to build asship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeat\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teech them ta long for the endless immensity of the eea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeap\n",
      "t you want to build a ship, don't arum up people to ether to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seap\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0:\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else:\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
